# üöÄ DRaFT-Q: Dynamic Rank-Aware Fine-Tuning for Quantized LLMs

**DRaFT-Q** is a novel Parameter-Efficient Fine-Tuning (PEFT) method for large language models (LLMs) that dynamically adjusts adapter **ranks per layer**, guided by real-time curvature signals ‚Äî all under a **strict VRAM budget** and compatible with **4-bit quantization**.

---

## üì¶ What‚Äôs in this Repository?

- üß† DRaFT-Q implementation (based on HuggingFace PEFT)
- üìä Training & evaluation scripts
- üìà Benchmarks, loss curves & GPU usage graphs
- üßÆ Theoretical proofs of optimal rank allocation & convergence
- ‚úÖ Fine-tuning on LoRA, QLoRA, and DRaFT-Q

---

## üß† Why DRaFT-Q?

| Problem with Fixed-Rank PEFT | DRaFT-Q‚Äôs Solution |
|------------------------------|---------------------|
| Static adapter rank wastes capacity | Curvature-based dynamic rank scaling |
| No control over memory growth | Rank rollback under strict VRAM cap |
| Poor fit for long-context / 4-bit | Fully compatible with QLoRA + FlashAttention |
| Overfitting in shallow layers | Adaptive rank shrinkage |

---

## üîç Method Comparison

| Feature                     | LoRA | QLoRA | AdaLoRA | üü¢ DRaFT-Q |
|-----------------------------|------|-------|---------|------------|
| Fixed Adapter Rank          | ‚úÖ    | ‚úÖ     | ‚ùå       | ‚ùå          |
| Dynamic Rank (per layer)    | ‚ùå    | ‚ùå     | ‚úÖ       | ‚úÖ          |
| Curvature-Based Scaling     | ‚ùå    | ‚ùå     | SVD/Hessian | ‚úÖ (EMA)   |
| 4-bit Quantization          | ‚ùå    | ‚úÖ     | ‚ùå       | ‚úÖ          |
| VRAM-Constrained Scheduling | ‚ùå    | ‚ùå     | ‚ùå       | ‚úÖ ‚úÖ        |
| Flash-Attention Compatible  | ‚úÖ    | ‚úÖ     | ‚úÖ       | ‚úÖ ‚úÖ        |

---

## üß™ Phase 2 Benchmark Results (100 Steps)

> Model: `tiiuae/falcon-rw-1b` | Task: AG-News Classification | GPU: T4 (15 GB)

| Adapter Type | Final Accuracy | GPU Peak |
|--------------|----------------|----------|
| **LoRA**     | 83.2%          | 5.84 GB  |
| **QLoRA**    | 84.1%          | 5.84 GB  |
| **DRaFT-Q**  | **86.4% ‚úÖ**    | **5.84 GB ‚úÖ** |

üü¢ DRaFT-Q maintained **equal VRAM** to QLoRA & LoRA, but achieved **+3.2% higher accuracy** and the most stable learning curve.

---

## üß¨ Trained Models

| Model Name                            | Params | Task            | Adapter Types Tested   | Notes                 |
|---------------------------------------|--------|------------------|-------------------------|-----------------------|
| `meta-llama/Llama-2-7b-hf`            | 7B     | Summarization    | LoRA, QLoRA, DRaFT-Q    | `news_summary_more.csv` |
| `tiiuae/falcon-rw-1b`                 | 1B     | Question Answering | LoRA, QLoRA, DRaFT-Q    | SQuAD v1.1            |
| `TinyLlama/TinyLlama-1.1B-Chat-v1.0`  | 1.1B   | Classification   | LoRA, QLoRA, DRaFT-Q    | AG-News               |

---

## üìö Datasets Used

| Dataset            | Type              | Link                                                                 |
|--------------------|-------------------|----------------------------------------------------------------------|
| NewsSummaryMore    | Summarization     | [`news_summary_more.csv`](https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv) |
| SQuAD v1.1         | QA (span-based)   | [`train-v1.1.json`](https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json) |
| AG-News            | Classification    | [`train.csv`](https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv) |

---

## üìà Visuals (Phase 2)

### üîπ DRaFT-Q vs LoRA vs QLoRA ‚Äì Accuracy (Falcon-rw-1b)
![Accuracy Benchmark](assets/falcon_accuracy.png)

### üîπ GPU Usage Graph ‚Äì DRaFT-Q
![DRaFT-Q GPU](assets/gpu_draftq.png)

### üîπ GPU Usage Graph ‚Äì LoRA
![LoRA GPU](assets/gpu_lora.png)

### üîπ GPU Usage Graph ‚Äì QLoRA
![QLoRA GPU](assets/gpu_qlora.png)

---

## üß† Theoretical Highlights

- **VRAM Budget Safety**
  DRaFT-Q rolls back ranks when adapter size exceeds user-defined GPU cap.

- **Convergence Lemma**
  DRaFT-Q converges to a local minimum if curvature signals are EMA-contracting.

---

## üõ†Ô∏è Installation & Training (Coming Soon)

